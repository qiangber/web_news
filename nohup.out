:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
2016-10-19 16:42:29 [scrapy] INFO: Scrapy 1.2.0 started (bot: web_news)
2016-10-19 16:42:29 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'web_news.spiders', 'SPIDER_MODULES': ['web_news.spiders'], 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS_PER_DOMAIN': 32, 'CONCURRENT_REQUESTS': 32, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'METAREFRESH_ENABLED': False, 'CONCURRENT_REQUESTS_PER_IP': 32, 'BOT_NAME': 'web_news', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'COMMANDS_MODULE': 'web_news.commands', 'REDIRECT_ENABLED': False, 'DOWNLOAD_DELAY': 0.25}
2016-10-19 16:42:29 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-10-19 16:42:29 [kejixun] INFO: get key 1
2016-10-19 16:42:29 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-10-19 16:42:29 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-10-19 16:42:29 [scrapy] INFO: Enabled item pipelines:
['web_news.pipelines.MongoDBPipeline']
2016-10-19 16:42:29 [scrapy] INFO: Spider opened
2016-10-19 16:42:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-10-19 16:43:02 [scrapy] INFO: Closing spider (finished)
2016-10-19 16:43:02 [kejixun] INFO: wait 1 spiders to stop
2016-10-19 16:43:02 [kejixun] INFO: all slave spider exit
2016-10-19 16:43:02 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13952,
 'downloader/request_count': 40,
 'downloader/request_method_count/GET': 40,
 'downloader/response_bytes': 431318,
 'downloader/response_count': 40,
 'downloader/response_status_count/200': 39,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 10, 19, 8, 43, 2, 297969),
 'item_scraped_count': 22,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 40,
 'scheduler/dequeued/redis': 40,
 'scheduler/enqueued/redis': 40,
 'start_time': datetime.datetime(2016, 10, 19, 8, 42, 29, 475239)}
2016-10-19 16:43:02 [scrapy] INFO: Spider closed (finished)
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 142, in execute
    _run_print_help(parser, _run_command, cmd, args, opts)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 88, in _run_print_help
    func(*a, **kw)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 149, in _run_command
    cmd.run(args, opts)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 162, in crawl
    crawler = self.create_crawler(crawler_or_spidercls)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 190, in create_crawler
    return self._create_crawler(crawler_or_spidercls)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 194, in _create_crawler
    spidercls = self.spider_loader.load(spidercls)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/spiderloader.py", line 43, in load
    raise KeyError("Spider not found: {}".format(spider_name))
KeyError: 'Spider not found: itms'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng1.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng1.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
Traceback (most recent call last):
      File "/usr/local/bin/scrapy", line 11, in <module>
sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
  File "/usr/local/bin/scrapy", line 11, in <module>
        sys.exit(execute())
sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
      File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
        StreamHandler.__init__(self, self._open())
StreamHandler.__init__(self, self._open())
StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
      File "/usr/lib/python2.7/codecs.py", line 878, in open
stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    StreamHandler.__init__(self, self._open())
    StreamHandler.__init__(self, self._open())
      File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
            file = __builtin__.open(filename, mode, buffering)
    file = __builtin__.open(filename, mode, buffering)
file = __builtin__.open(filename, mode, buffering)
    file = __builtin__.open(filename, mode, buffering)
    file = __builtin__.open(filename, mode, buffering)
IOErrorIOErrorfile = __builtin__.open(filename, mode, buffering)
IOErrorIOError: : : [Errno 2] No such file or directory: '/home/u234/web_news/log/tianya10.log': IOError[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya1.log'IOError[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya6.log'
[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya5.log'
: 
: 
[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya8.log'[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya4.log'

    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tianya9.log'
    file = __builtin__.open(filename, mode, buffering)
        file = __builtin__.open(filename, mode, buffering)
file = __builtin__.open(filename, mode, buffering)
IOErrorIOError: : [Errno 2] No such file or directory: '/home/u234/web_news/log/tianya2.log'[Errno 2] No such file or directory: '/home/u234/web_news/log/tianya7.log'

IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tianya3.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng9.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/ifeng2.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews4.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews5.log'
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews7.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews6.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews1.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/sznews9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan1.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan4.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan7.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/yunyan5.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
Traceback (most recent call last):
  File "/usr/lib/python2.7/codecs.py", line 878, in open
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
        handler = _get_handler(settings)
file = __builtin__.open(filename, mode, buffering)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj10.log'
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj8.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysdj9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb4.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
      File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
    IOErrorstream = codecs.open(self.baseFilename, self.mode, self.encoding)
:   File "/usr/lib/python2.7/codecs.py", line 878, in open
[Errno 2] No such file or directory: '/home/u234/web_news/log/gywb9.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gywb5.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj5.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj7.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/nmdj3.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw7.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw5.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw1.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw8.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    file = __builtin__.open(filename, mode, buffering)
IOError    handler = _get_handler(settings)
:   File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
[Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw3.log'
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/gysjw9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    cmd.crawler_process = CrawlerProcess(settings)
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu7.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu9.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu2.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huanqiu8.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
        cmd.crawler_process = CrawlerProcess(settings)
sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc2.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc1.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc8.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    Traceback (most recent call last):
handler = _get_handler(settings)
  File "/usr/local/bin/scrapy", line 11, in <module>
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    sys.exit(execute())
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc7.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/itmsc4.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen2.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/techxinwen9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn4.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn8.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn6.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/southcn7.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews1.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjnews10.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun10.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun7.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun9.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/kejixun8.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews10.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/cnetnews9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews2.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews1.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/citnews3.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd7.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/bjd2.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6184.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6187.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k61810.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6189.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6185.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6182.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    file = __builtin__.open(filename, mode, buffering)
    IOErrorconfigure_logging(self.settings)
:   File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
[Errno 2] No such file or directory: '/home/u234/web_news/log/k6183.log'
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6188.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6186.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/k6181.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn1.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn9.log'
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
    StreamHandler.__init__(self, self._open())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn2.log'
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn7.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/21cn10.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1005.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1004.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1009.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi10010.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1006.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1001.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1003.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1002.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1007.log'
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/huaxi1008.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren1.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren5.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren7.log'
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/tongren9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan6.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan4.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan1.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan3.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan9.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/qiannan8.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
Traceback (most recent call last):
IOError  File "/usr/local/bin/scrapy", line 11, in <module>
: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi3.log'
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi8.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi4.log'
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi6.log'
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi10.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi1.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi7.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/zunyi9.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun6.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun3.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    file = __builtin__.open(filename, mode, buffering)
IOError    : sys.exit(execute())
[Errno 2] No such file or directory: '/home/u234/web_news/log/anshun1.log'
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun4.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun2.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun9.log'
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun5.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
Traceback (most recent call last):
IOError  File "/usr/local/bin/scrapy", line 11, in <module>
: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun7.log'
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun8.log'
Traceback (most recent call last):
  File "/usr/local/bin/scrapy", line 11, in <module>
    sys.exit(execute())
  File "/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py", line 141, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 240, in __init__
    configure_logging(self.settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 100, in configure_logging
    handler = _get_handler(settings)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/log.py", line 109, in _get_handler
    handler = logging.FileHandler(filename, encoding=encoding)
  File "/usr/lib/python2.7/logging/__init__.py", line 903, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python2.7/logging/__init__.py", line 930, in _open
    stream = codecs.open(self.baseFilename, self.mode, self.encoding)
  File "/usr/lib/python2.7/codecs.py", line 878, in open
    file = __builtin__.open(filename, mode, buffering)
IOError: [Errno 2] No such file or directory: '/home/u234/web_news/log/anshun10.log'
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/forum.py", line 46, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/forum.py", line 19, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1331, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1185, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 71, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 94, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 43, in from_crawler
    spider.compete_key()
  File "/home/u234/web_news/web_news/misc/spiderredis.py", line 16, in compete_key
    while self.server.sadd(self.redis_compete, self.key) == 0:
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 1494, in sadd
    return self.execute_command('SADD', name, *values)
  File "/usr/local/lib/python2.7/dist-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/usr/local/lib/python2.7/dist-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error connecting to 192.168.1.54:6379. timed out.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named pyasn1.codec.der.decoder'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
/home/u234/web_news/web_news/spiders/chinanews.py:3: ScrapyDeprecationWarning: Module `scrapy.linkextractor` is deprecated, use `scrapy.linkextractors` instead
  from scrapy.linkextractor import LinkExtractor
/home/u234/web_news/web_news/spiders/gog_spider.py:4: ScrapyDeprecationWarning: Module `scrapy.spider` is deprecated, use `scrapy.spiders` instead
  from scrapy.spider import Spider
